{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.environ[\"HOME\"] + '/tmp_projects/reproduction-soft-qlearning-mutual-information')\n",
    "sys.path.append(os.environ[\"HOME\"] + '/tmp_projects/reproduction-soft-qlearning-mutual-information/tabular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular.agent import RandomAgent, QLAgent, SQLAgent, MIRLAgent, SQL_mAgent\n",
    "from tabular.environment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular.main import eval_snapshot, plot_results, plot_correct_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import frozendict\n",
    "import random\n",
    "import os\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world1 = {\n",
    "    'size': (3, 20),\n",
    "    'reward': (1, 18),\n",
    "    'walls': []\n",
    "}\n",
    "\n",
    "world2 = {\n",
    "    'size': (8, 8),\n",
    "    'reward': (3, 5),\n",
    "    'walls': [(2, 4), (2, 5), (3, 4), (4, 4), (4, 5)]\n",
    "}\n",
    "\n",
    "config = frozendict.frozendict({\n",
    "    'eps_train': 0.1,\n",
    "    'eps_eval': 0.05,\n",
    "    'gamma': 0.99,\n",
    "    'rho_lr': 2e-3,\n",
    "    'beta_lr': 2e-3,   # use?\n",
    "    'c': 1e-3,\n",
    "    'omega': 0.8,\n",
    "    'max_steps': 10000\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_snapshot(agent_class, agent_checkpoint, world_conf):\n",
    "    '''\n",
    "    described in the paper: \"The evaluation for a single snapshot is conducted by running the policy for \n",
    "    30 episodes lasting at most 100 environment steps. The epsilon value when in evaluation mode is set to 0.05.   \n",
    "    Every individual experiment is repeated with 10 different initial random seeds and results are averaged\"\n",
    "    '''\n",
    "    n_seeds = 10\n",
    "    n_episodes = 30\n",
    "    max_t = 100\n",
    "    rewards = list()\n",
    "    \n",
    "    env = GridWorld(world_conf)\n",
    "    agent = agent_class(env.get_actions(), config, **agent_checkpoint, mode='eval')\n",
    "    \n",
    "    for i_seed in range(n_seeds):\n",
    "        np.random.seed()\n",
    "        seed_results = list()\n",
    "        \n",
    "        for i_episode in range(n_episodes):\n",
    "            episode_reward = 0\n",
    "            obs = env.reset()\n",
    "            \n",
    "            for t in range(max_t):\n",
    "                action = agent.choose_action(obs)\n",
    "                reward, next_obs, done = env.action(action)\n",
    "                \n",
    "                obs = next_obs\n",
    "                episode_reward += reward\n",
    "    \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            seed_results.append(episode_reward)\n",
    "            \n",
    "        rewards.append(np.mean(seed_results))\n",
    "      \n",
    "    return np.mean(rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation_data will store info for plot 1\n",
    "evaluation_data = defaultdict(dict)\n",
    "training_data = defaultdict(dict)\n",
    "\n",
    "envs = {\n",
    "    'Grid_World_3x20': world1,\n",
    "    'Grid_World_8x8': world2\n",
    "}\n",
    "\n",
    "# storing info for plot 2\n",
    "correct_info = defaultdict(dict)\n",
    "\n",
    "exp_name = '10k_sqlm'\n",
    "ROOT_DIR = '/home/caleml/grotile'\n",
    "\n",
    "log_folder = ROOT_DIR + '/tabular/logs/%s_%s' % (datetime.datetime.now().strftime('%Y%m%d_%H%M%S'), exp_name)\n",
    "log_step = 1000\n",
    "\n",
    "for envname, envconf in envs.items():\n",
    "    env = GridWorld(envconf)\n",
    "        \n",
    "    for agent_class in (SQL_mAgent, QLAgent, SQLAgent, MIRLAgent):\n",
    "        log_subfolder = os.path.join(log_folder, envname.replace(' ', '_'), agent_class.__name__)\n",
    "        os.makedirs(log_subfolder)\n",
    "        \n",
    "        with open(os.path.join(log_subfolder, 'logs.csv'), 'w+') as f_log:\n",
    "            print('\\nTraining agent %s' % agent_class.__name__)\n",
    "            agent = agent_class(env.get_actions(), config)\n",
    "\n",
    "            total_reward = 0\n",
    "            obs = env.reset()\n",
    "            evals = dict()\n",
    "            train = dict()\n",
    "            correct_action = list()\n",
    "\n",
    "            for global_step in range(config.max_steps):\n",
    "                action = agent.choose_action(obs)\n",
    "                reward, next_obs, done = env.action(action)\n",
    "\n",
    "                agent.update(obs, action, reward, next_obs, done)\n",
    "\n",
    "                if (global_step + 1) % 100 == 0:\n",
    "                    eval_reward = eval_snapshot(agent_class, agent.get_checkpoint(), envconf)\n",
    "                    evals[global_step] = eval_reward\n",
    "                    print('Step %s, reward eval %s' % (global_step + 1, eval_reward))\n",
    "\n",
    "                if (global_step + 1) % log_step == 0:\n",
    "                    agent.log(log_subfolder, global_step)\n",
    "                    log_vars = ','.join([str(global_step), str(eval_reward)] + agent.get_current_vars()) + '\\n'\n",
    "                    f_log.write(log_vars)\n",
    "                    \n",
    "                # correct action information (different evaluation for both gridworlds)\n",
    "                if envname == 'Grid World 3x20':        # log correct action at each step\n",
    "                    correct_action.append(1 if action == 1 else 0)\n",
    "                elif envname == 'Grid World 8x8':       # log correct action at state (3,6)\n",
    "                    x_agent, y_agent = np.where(obs == 3)\n",
    "                    if (x_agent[0], y_agent[0]) == (3, 6):\n",
    "                        correct_action.append(1 if action == 3 else 0)\n",
    "                    \n",
    "                total_reward += reward\n",
    "                obs = next_obs\n",
    "                # env.display()\n",
    "                \n",
    "                if done:\n",
    "                    # print(\"Env done with reward %s\" % total_reward)\n",
    "                    train[global_step] = total_reward\n",
    "                    total_reward = 0\n",
    "                    obs = env.reset()\n",
    "\n",
    "        evaluation_data[envname][agent_class.__name__] = evals\n",
    "        training_data[envname][agent_class.__name__] = train\n",
    "        correct_info[envname][agent_class.__name__] = correct_action\n",
    "            \n",
    "        with open(os.path.join(log_subfolder, 'eval_data.json'), 'w+') as f_dump:\n",
    "            f_dump.write(json.dumps(evals))\n",
    "        \n",
    "        with open(os.path.join(log_subfolder, 'correct_action.json'), 'w+') as f_correct:\n",
    "            f_correct.write(json.dumps({'data': correct_action}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_colors = {\n",
    "    'QLAgent': 'g',\n",
    "    'SQLAgent': 'orangered',\n",
    "    'SQL_mAgent': 'orange',\n",
    "    'MIRLAgent': 'royalblue'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(evaluation_data, log_folder):\n",
    "    for envname, env_eval_data in evaluation_data.items():\n",
    "        for agent_type, eval_data in env_eval_data.items():\n",
    "            data_x = np.array(list(eval_data.keys()))\n",
    "            data_y = np.array(list(eval_data.values()))\n",
    "            x_smooth = np.linspace(data_x.min(), data_x.max(), 200)\n",
    "            f_smooth = interp1d(data_x, data_y, kind='cubic')\n",
    "            df = pd.Series(data_y)\n",
    "            plt.plot(data_x, df.ewm(span=10).mean(), paper_colors[agent_type], label='%s' % agent_type.replace('Agent', ''))\n",
    "\n",
    "            # plt.plot(x_smooth, f_smooth(x_smooth), paper_colors[agent_type], label='%s eval RAW' % agent_type, alpha=0.4)\n",
    "            # plt.plot(ewma(data_x, span=1000), label='%s train EWMA' % agent_type)\n",
    "\n",
    "        plt.xlabel('environment interactions')\n",
    "        plt.ylabel('reward')\n",
    "        plt.legend()\n",
    "        plt.title(envname)\n",
    "        \n",
    "        if log_folder:\n",
    "            plt.savefig(log_folder + '/results_%s.png' % envname)\n",
    "            \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correct action\n",
    "\n",
    "def plot_correct_action(correct_info, log_folder=None):\n",
    "    labels = {\n",
    "        'Grid_World_3x20': {\n",
    "            'title': 'Correct action while training',\n",
    "            'x_label': 'Training step',\n",
    "            'y_label': '1: Correct action (right), 0: Incorrect action'\n",
    "        },\n",
    "        'Grid_World_8x8': {\n",
    "            'title': 'Correct Infrequent action',\n",
    "            'x_label': 'Interactions in state (3, 6)',\n",
    "            'y_label': '1: Correct action (left), 0: Incorrect action'\n",
    "        } \n",
    "    }\n",
    "\n",
    "    for envname, correct_data in correct_info.items():\n",
    "        envname = envname.replace(' ', '_')\n",
    "        for agent_type, correct_action in correct_data.items():\n",
    "            data_x = np.arange(0, len(correct_action), 1)\n",
    "            data_y = np.array(correct_action)\n",
    "            df = pd.Series(correct_action)\n",
    "            plt.plot(data_x, df.ewm(span=1000).mean(), paper_colors[agent_type], label=agent_type.replace('Agent', ''))\n",
    "\n",
    "        plt.xlabel(labels[envname]['x_label'])\n",
    "        plt.ylabel(labels[envname]['y_label'])\n",
    "        plt.legend()\n",
    "        plt.title(labels[envname]['title'])\n",
    "        \n",
    "        if log_folder:\n",
    "            plt.savefig(log_folder + '/actions_%s.png' % envname)\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correct_action(correct_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# plot existing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = \"/Users/voltali/tmp_projects/grotile/tabular/logs/20190104_230535_all_10k\"\n",
    "\n",
    "env_names = ['Grid_World_3x20', 'Grid_World_8x8']\n",
    "agent_names = ['QLAgent', 'SQLAgent', 'SQL_mAgent', 'MIRLAgent']\n",
    "\n",
    "eval_data = defaultdict(dict)\n",
    "correct_data = defaultdict(dict)\n",
    "\n",
    "for envname in env_names:\n",
    "    for agentname in agent_names:\n",
    "        data_path = os.path.join(experiment_dir, envname, agentname, 'eval_data.json')\n",
    "        with open(data_path, 'r') as f_eval:\n",
    "            data = json.loads(f_eval.read())\n",
    "            eval_data[envname][agentname] = {int(k): v for k, v in data.items()}\n",
    "            \n",
    "        correct_action_path = os.path.join(experiment_dir, envname, agentname, 'correct_action.json')\n",
    "        with open(correct_action_path, 'r') as f_correct:\n",
    "            data = json.loads(f_correct.read())\n",
    "            correct_data[envname][agentname] = data['data']\n",
    "            \n",
    "plot_results(eval_data)\n",
    "plot_correct_action(correct_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot experiments from different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100k MIRL on 3x20 and 8x8 (old)\n",
    "mirl = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results/20190104_203749_100k_mirl\"\n",
    "\n",
    "# 100k all on 8x8\n",
    "all_8x8 = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results/20190106_161824_100k_all_8x8\"\n",
    "\n",
    "# 100k all on 3x20\n",
    "all_3x20 = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results/20190105_020922_all_100k\"\n",
    "\n",
    "paths = {\n",
    "    'Grid_World_3x20': {\n",
    "        'QLAgent': all_3x20,\n",
    "        'SQLAgent': all_3x20,\n",
    "        'SQL_mAgent': all_3x20,\n",
    "        'MIRLAgent': mirl\n",
    "    },\n",
    "    'Grid_World_8x8': {\n",
    "        'QLAgent': all_8x8,\n",
    "        'SQLAgent': all_8x8,\n",
    "        'SQL_mAgent': all_8x8,\n",
    "        'MIRLAgent': mirl\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_data = defaultdict(dict)\n",
    "correct_data = defaultdict(dict)\n",
    "\n",
    "savedir = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results\"\n",
    "\n",
    "for envname, data in paths.items():\n",
    "    for agentname, basedir in data.items():\n",
    "        data_path = os.path.join(basedir, envname, agentname, 'eval_data.json')\n",
    "        with open(data_path, 'r') as f_eval:\n",
    "            data = json.loads(f_eval.read())\n",
    "            eval_data[envname][agentname] = {int(k): v for k, v in data.items()}\n",
    "            \n",
    "        correct_action_path = os.path.join(basedir, envname, agentname, 'correct_action.json')\n",
    "        with open(correct_action_path, 'r') as f_correct:\n",
    "            data = json.loads(f_correct.read())\n",
    "            correct_data[envname][agentname] = data['data']\n",
    "            \n",
    "plot_results(eval_data, log_folder=savedir)\n",
    "plot_correct_action(correct_data, log_folder=savedir)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
