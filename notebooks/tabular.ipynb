{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.environ[\"HOME\"] + '/tmp_projects/reproduction-soft-qlearning-mutual-information')\n",
    "sys.path.append(os.environ[\"HOME\"] + '/tmp_projects/reproduction-soft-qlearning-mutual-information/tabular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular.agent import RandomAgent, QLAgent, SQLAgent, MIRLAgent, SQL_mAgent\n",
    "from tabular.environment import GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabular.main import eval_snapshot, plot_results, plot_correct_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "import frozendict\n",
    "import random\n",
    "import os\n",
    "from pprint import pprint\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "world1 = {\n",
    "    'size': (3, 20),\n",
    "    'reward': (1, 18),\n",
    "    'walls': []\n",
    "}\n",
    "\n",
    "world2 = {\n",
    "    'size': (8, 8),\n",
    "    'reward': (3, 5),\n",
    "    'walls': [(2, 4), (2, 5), (3, 4), (4, 4), (4, 5)]\n",
    "}\n",
    "\n",
    "config = frozendict.frozendict({\n",
    "    'eps_train': 0.1,\n",
    "    'eps_eval': 0.05,\n",
    "    'gamma': 0.999,\n",
    "    'rho_lr': 2e-3,\n",
    "    'beta_lr': 2e-3,   # use?\n",
    "    'c': 1e-3,\n",
    "    'omega': 0.8,\n",
    "    'max_steps': 10000\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training agent MIRLAgent\n",
      "Step 100, reward eval -82.57000000000001\n",
      "Step 200, reward eval -76.38000000000001\n",
      "Step 300, reward eval -77.86999999999999\n",
      "Step 400, reward eval -76.55666666666666\n",
      "Step 500, reward eval -80.51333333333332\n",
      "Step 600, reward eval -73.80333333333333\n",
      "Step 700, reward eval -69.38666666666667\n",
      "Step 800, reward eval -74.80333333333333\n",
      "Step 900, reward eval -64.34666666666666\n",
      "Step 1000, reward eval -68.70333333333333\n",
      "Step 1100, reward eval -71.14000000000001\n",
      "Step 1200, reward eval -63.633333333333326\n",
      "Step 1300, reward eval -67.16333333333334\n",
      "Step 1400, reward eval -68.7\n",
      "Step 1500, reward eval -61.67999999999999\n",
      "Step 1600, reward eval -59.973333333333336\n",
      "Step 1700, reward eval -60.75333333333333\n",
      "Step 1800, reward eval -58.966666666666676\n",
      "Step 1900, reward eval -58.6\n",
      "Step 2000, reward eval -60.55\n",
      "Step 2100, reward eval -50.43666666666667\n",
      "Step 2200, reward eval -48.336666666666666\n",
      "Step 2300, reward eval -57.03000000000001\n",
      "Step 2400, reward eval -60.73\n",
      "Step 2500, reward eval -50.230000000000004\n",
      "Step 2600, reward eval -56.466666666666676\n",
      "Step 2700, reward eval -51.239999999999995\n",
      "Step 2800, reward eval -51.019999999999996\n",
      "Step 2900, reward eval -52.03000000000001\n",
      "Step 3000, reward eval -51.8\n",
      "Step 3100, reward eval -52.81333333333333\n",
      "Step 3200, reward eval -53.38666666666667\n",
      "Step 3300, reward eval -51.966666666666676\n",
      "Step 3400, reward eval -48.68666666666667\n",
      "Step 3500, reward eval -47.91333333333334\n",
      "Step 3600, reward eval -46.086666666666666\n",
      "Step 3700, reward eval -47.56333333333333\n",
      "Step 3800, reward eval -46.90333333333333\n",
      "Step 3900, reward eval -50.81666666666666\n",
      "Step 4000, reward eval -44.86\n",
      "Step 4100, reward eval -51.10333333333334\n",
      "Step 4200, reward eval -51.239999999999995\n"
     ]
    }
   ],
   "source": [
    "# evaluation_data will store info for plot 1\n",
    "evaluation_data = defaultdict(dict)\n",
    "training_data = defaultdict(dict)\n",
    "\n",
    "envs = {\n",
    "    'Grid_World_3x20': world1,\n",
    "    'Grid_World_8x8': world2\n",
    "}\n",
    "# agents = (SQL_mAgent, QLAgent, SQLAgent, MIRLAgent)\n",
    "agents = (MIRLAgent,)\n",
    "\n",
    "# storing info for plot 2\n",
    "correct_info = defaultdict(dict)\n",
    "\n",
    "exp_name = '10k_sqlm'\n",
    "ROOT_DIR = os.environ[\"HOME\"] + '/tmp_projects/reproduction-soft-qlearning-mutual-information'\n",
    "\n",
    "log_folder = ROOT_DIR + '/tabular/logs/%s_%s' % (datetime.datetime.now().strftime('%Y%m%d_%H%M%S'), exp_name)\n",
    "log_step = 1000\n",
    "\n",
    "for envname, envconf in envs.items():\n",
    "    env = GridWorld(envconf)\n",
    "        \n",
    "    for agent_class in agents:\n",
    "        log_subfolder = os.path.join(log_folder, envname.replace(' ', '_'), agent_class.__name__)\n",
    "        os.makedirs(log_subfolder)\n",
    "        \n",
    "        with open(os.path.join(log_subfolder, 'logs.csv'), 'w+') as f_log:\n",
    "            print('\\nTraining agent %s' % agent_class.__name__)\n",
    "            agent = agent_class(env.get_actions(), config)\n",
    "\n",
    "            total_reward = 0\n",
    "            obs = env.reset()\n",
    "            evals = dict()\n",
    "            train = dict()\n",
    "            correct_action = list()\n",
    "\n",
    "            for global_step in range(config.max_steps):\n",
    "                action = agent.choose_action(obs)\n",
    "                reward, next_obs, done = env.action(action)\n",
    "\n",
    "                agent.update(obs, action, reward, next_obs, done)\n",
    "\n",
    "                if (global_step + 1) % 100 == 0:\n",
    "                    eval_reward = eval_snapshot(agent_class, agent.get_checkpoint(), envconf, config)\n",
    "                    evals[global_step] = eval_reward\n",
    "                    print('Step %s, reward eval %s' % (global_step + 1, eval_reward))\n",
    "\n",
    "                if (global_step + 1) % log_step == 0:\n",
    "                    agent.log(log_subfolder, global_step)\n",
    "                    log_vars = ','.join([str(global_step), str(eval_reward)] + agent.get_current_vars()) + '\\n'\n",
    "                    f_log.write(log_vars)\n",
    "                    \n",
    "                # correct action information (different evaluation for both gridworlds)\n",
    "                if envname == 'Grid_World_3x20':        # log correct action at each step\n",
    "                    correct_action.append(1 if action == 1 else 0)\n",
    "                elif envname == 'Grid_World_8x8':       # log correct action at state (3,6)\n",
    "                    x_agent, y_agent = np.where(obs == 3)\n",
    "                    if (x_agent[0], y_agent[0]) == (3, 6):\n",
    "                        correct_action.append(1 if action == 3 else 0)\n",
    "                    \n",
    "                total_reward += reward\n",
    "                obs = next_obs\n",
    "                # env.display()\n",
    "                \n",
    "                if done:\n",
    "                    # print(\"Env done with reward %s\" % total_reward)\n",
    "                    train[global_step] = total_reward\n",
    "                    total_reward = 0\n",
    "                    obs = env.reset()\n",
    "\n",
    "        evaluation_data[envname][agent_class.__name__] = evals\n",
    "        training_data[envname][agent_class.__name__] = train\n",
    "        correct_info[envname][agent_class.__name__] = correct_action\n",
    "            \n",
    "        with open(os.path.join(log_subfolder, 'eval_data.json'), 'w+') as f_dump:\n",
    "            f_dump.write(json.dumps(evals))\n",
    "        \n",
    "        with open(os.path.join(log_subfolder, 'correct_action.json'), 'w+') as f_correct:\n",
    "            f_correct.write(json.dumps({'data': correct_action}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_colors = {\n",
    "    'QLAgent': 'g',\n",
    "    'SQLAgent': 'orangered',\n",
    "    'SQL_mAgent': 'orange',\n",
    "    'MIRLAgent': 'royalblue'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(evaluation_data, log_folder):\n",
    "    for envname, env_eval_data in evaluation_data.items():\n",
    "        for agent_type, eval_data in env_eval_data.items():\n",
    "            data_x = np.array(list(eval_data.keys()))\n",
    "            data_y = np.array(list(eval_data.values()))\n",
    "            x_smooth = np.linspace(data_x.min(), data_x.max(), 200)\n",
    "            f_smooth = interp1d(data_x, data_y, kind='cubic')\n",
    "            df = pd.Series(data_y)\n",
    "            plt.plot(data_x, df.ewm(span=10).mean(), paper_colors[agent_type], label='%s' % agent_type.replace('Agent', ''))\n",
    "\n",
    "            # plt.plot(x_smooth, f_smooth(x_smooth), paper_colors[agent_type], label='%s eval RAW' % agent_type, alpha=0.4)\n",
    "            # plt.plot(ewma(data_x, span=1000), label='%s train EWMA' % agent_type)\n",
    "\n",
    "        plt.xlabel('environment interactions')\n",
    "        plt.ylabel('reward')\n",
    "        plt.legend()\n",
    "        plt.title(envname)\n",
    "        \n",
    "        if log_folder:\n",
    "            plt.savefig(log_folder + '/results_%s.png' % envname)\n",
    "            \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correct action\n",
    "\n",
    "def plot_correct_action(correct_info, log_folder=None):\n",
    "    labels = {\n",
    "        'Grid_World_3x20': {\n",
    "            'title': 'Correct action while training',\n",
    "            'x_label': 'Training step',\n",
    "            'y_label': '1: Correct action (right), 0: Incorrect action'\n",
    "        },\n",
    "        'Grid_World_8x8': {\n",
    "            'title': 'Correct Infrequent action',\n",
    "            'x_label': 'Interactions in state (3, 6)',\n",
    "            'y_label': '1: Correct action (left), 0: Incorrect action'\n",
    "        } \n",
    "    }\n",
    "\n",
    "    for envname, correct_data in correct_info.items():\n",
    "        envname = envname.replace(' ', '_')\n",
    "        for agent_type, correct_action in correct_data.items():\n",
    "            data_x = np.arange(0, len(correct_action), 1)\n",
    "            data_y = np.array(correct_action)\n",
    "            df = pd.Series(correct_action)\n",
    "            plt.plot(data_x, df.ewm(span=1000).mean(), paper_colors[agent_type], label=agent_type.replace('Agent', ''))\n",
    "\n",
    "        plt.xlabel(labels[envname]['x_label'])\n",
    "        plt.ylabel(labels[envname]['y_label'])\n",
    "        plt.legend()\n",
    "        plt.title(labels[envname]['title'])\n",
    "        \n",
    "        if log_folder:\n",
    "            plt.savefig(log_folder + '/actions_%s.png' % envname)\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correct_action(correct_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# plot existing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = \"/Users/voltali/tmp_projects/grotile/tabular/logs/20190104_230535_all_10k\"\n",
    "\n",
    "env_names = ['Grid_World_3x20', 'Grid_World_8x8']\n",
    "agent_names = ['QLAgent', 'SQLAgent', 'SQL_mAgent', 'MIRLAgent']\n",
    "\n",
    "eval_data = defaultdict(dict)\n",
    "correct_data = defaultdict(dict)\n",
    "\n",
    "for envname in env_names:\n",
    "    for agentname in agent_names:\n",
    "        data_path = os.path.join(experiment_dir, envname, agentname, 'eval_data.json')\n",
    "        with open(data_path, 'r') as f_eval:\n",
    "            data = json.loads(f_eval.read())\n",
    "            eval_data[envname][agentname] = {int(k): v for k, v in data.items()}\n",
    "            \n",
    "        correct_action_path = os.path.join(experiment_dir, envname, agentname, 'correct_action.json')\n",
    "        with open(correct_action_path, 'r') as f_correct:\n",
    "            data = json.loads(f_correct.read())\n",
    "            correct_data[envname][agentname] = data['data']\n",
    "            \n",
    "plot_results(eval_data)\n",
    "plot_correct_action(correct_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot experiments from different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100k MIRL on 3x20 and 8x8 (old)\n",
    "mirl = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results/20190104_203749_100k_mirl\"\n",
    "\n",
    "# 100k all on 8x8\n",
    "all_8x8 = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results/20190106_161824_100k_all_8x8\"\n",
    "\n",
    "# 100k all on 3x20\n",
    "all_3x20 = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results/20190105_020922_all_100k\"\n",
    "\n",
    "# 10k experiments\n",
    "all_10k = \"/Users/voltali/tmp_projects/reproduction-soft-qlearning-mutual-information/tabular/logs/20190104_230535_all_10k\"\n",
    "mirl_10k = \"\"\n",
    "\n",
    "paths_old = {\n",
    "    'Grid_World_3x20': {\n",
    "        'QLAgent': all_3x20,\n",
    "        'SQLAgent': all_3x20,\n",
    "        'SQL_mAgent': all_3x20,\n",
    "        'MIRLAgent': mirl\n",
    "    },\n",
    "    'Grid_World_8x8': {\n",
    "        'QLAgent': all_8x8,\n",
    "        'SQLAgent': all_8x8,\n",
    "        'SQL_mAgent': all_8x8,\n",
    "        'MIRLAgent': mirl\n",
    "    }\n",
    "}\n",
    "\n",
    "paths = {\n",
    "    'Grid_World_3x20': {\n",
    "        'QLAgent': all_10k,\n",
    "        'SQLAgent': all_10k,\n",
    "        'SQL_mAgent': all_10k,\n",
    "        'MIRLAgent': mirl_10k\n",
    "    },\n",
    "    'Grid_World_8x8': {\n",
    "        'QLAgent': all_10k,\n",
    "        'SQLAgent': all_10k,\n",
    "        'SQL_mAgent': all_10k,\n",
    "        'MIRLAgent': mirl_10k\n",
    "    }\n",
    "}\n",
    "\n",
    "eval_data = defaultdict(dict)\n",
    "correct_data = defaultdict(dict)\n",
    "\n",
    "savedir = \"/Users/voltali/tmp_projects/grotile/tabular/logs/results\"\n",
    "\n",
    "for envname, data in paths.items():\n",
    "    for agentname, basedir in data.items():\n",
    "        data_path = os.path.join(basedir, envname, agentname, 'eval_data.json')\n",
    "        with open(data_path, 'r') as f_eval:\n",
    "            data = json.loads(f_eval.read())\n",
    "            eval_data[envname][agentname] = {int(k): v for k, v in data.items()}\n",
    "            \n",
    "        correct_action_path = os.path.join(basedir, envname, agentname, 'correct_action.json')\n",
    "        with open(correct_action_path, 'r') as f_correct:\n",
    "            data = json.loads(f_correct.read())\n",
    "            correct_data[envname][agentname] = data['data']\n",
    "            \n",
    "plot_results(eval_data, log_folder=savedir)\n",
    "plot_correct_action(correct_data, log_folder=savedir)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
